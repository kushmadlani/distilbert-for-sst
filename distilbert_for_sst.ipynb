{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distilbert_for_sst.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPnTTTnfCWVB",
        "colab_type": "text"
      },
      "source": [
        "The Transformer is the latest advance in Deep Learning architectures that has driven most state-of-the-art progress in NLP since it was first presented in ['Attention is All You Need'](https://arxiv.org/abs/1706.03762). Since then, ever larger models are being made, with parameters running into the billions. \n",
        "\n",
        "> Side-note: I think we're inflection point in ML with OpenAI's release of their API - everyone now has easy access to these state-of-the-art language models, we're gonna see an explosion of use-cases + value creation\n",
        "\n",
        "\n",
        "There's a lot of greats resources with visualisations to help understand the architecture which I'll come back to. First, a brief introduction to what makes Transformers so powerful:\n",
        "\n",
        "*   *Self-attention*: a mechanism allowing us to learn contextual relationships between different elements in our input sequence, replacing the need for sequential structure (from RNN/LSTM cells).\n",
        "*   *Multi-headed attention*: multiple heads of the model carry out self-attnetion, attending to information jointly at different parts of the sequence from different subspaces. This allows us to learn a variety of features of language + means the model can scale efficiently with large datasets + unsupervised learning.\n",
        "* *Transfer learning*: Transformers use the knowledge extracted from a prior setting (usually in the form a language model), which can be unsupervised, then apply or *transfer* to a specific domain, where labelled data is available. This allows a large rich corpus of text to be used in the first pre-training stage, before the model is fine-tuned on custom data. \n",
        "\n",
        "*insert pre training photo*\n",
        "\n",
        "In this post, we'll look at how to fine tune a pre-trained model for the task fo sentiment analysis using Hugging Face's [Transformer](https://huggingface.co/transformers/pretrained_models.html) library, that gives simple access to many of the top transformed-based models (*BERT*, *GPT-2*, *XLNet* etc).  We'll use *DistilBert* here, a lightweight version of the famous *BERT* model with 66 million parameters that's slightly easier to run on a single Colab GPU.\n",
        "\n",
        "BERT stands for Bidirectional Encoder Representations from Transformers. It uses a *masked* language model where 15% of a sequence's tokens are randomly masked, then the model learns to predict, given a token, what came before *or* after it (the bi-dircectional part). In addition, it has a next sentence prediction objective (did this sentence come after a previous one). BERT differs from a more standard *casual* language model, that predicts the most likely next token in the sequence in a left-to-right direction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EXPFuR-CakA",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlN_8b6FVTPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "4dff9edc-fa81-4165-bb90-19deff538b39"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw1lTXEJo5Fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive # import drive from google colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDLWrlLoo6TA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1898f03b-ff12-4647-e393-98f90bd22754"
      },
      "source": [
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sswx8jnjUh-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f3584497-77d2-4a9b-fb27-9c2dfb83d747"
      },
      "source": [
        "import transformers\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from os import path\n",
        "import requests\n",
        "import gzip\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "RANDOM_SEED = 0\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN_IQYuGCgdF",
        "colab_type": "text"
      },
      "source": [
        "## Loading our Data\n",
        "\n",
        "For the task of sentiment analysis our model takes a sentence as input and outputs one of five classes representing sentiments (very negative, negative, neutral, positive, very positive). The Stanford Sentiment Treebank (SST-5) is the best-known dataset for this, composed of 11855 such sentences with labels 1-5 already split into train, validation and test sets (of sizes 8544, 1101 and 2210). \n",
        "\n",
        "Let's download the dataset, then split into train/val/test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekH4chUMaG29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to download the data\n",
        "def downloadFile(url,filepath):\n",
        "    if not path.exists(filepath) :\n",
        "        with requests.get(url) as r :\n",
        "            open(filepath, 'wb').write(r.content)\n",
        "    if not path.exists(filepath[:-4]) :\n",
        "        with zipfile.ZipFile(filepath,'r') as zp :\n",
        "            zp.extractall()\n",
        "\n",
        "# format the data, extracting the sentence as\n",
        "# well as the sentiment of the entire sentence\n",
        "def ReadTextFile(filepath) :\n",
        "    y = []\n",
        "    X = []\n",
        "    with open(filepath) as r :\n",
        "        for line in r.read().split('\\n') :\n",
        "            #set_trace()\n",
        "            if len(line)==0 :\n",
        "                pass\n",
        "            else :\n",
        "                y.append(int(line[1]))\n",
        "                X.append([word[:-1].replace(')','') for word in line.split() if word[-1]==')'])   \n",
        "    return y, X\n",
        "\n",
        "downloadFile('https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip', 'trainDevTestTrees_PTB.zip')\n",
        "\n",
        "y_train, X_train = ReadTextFile(\"./trees/train.txt\")\n",
        "y_val, X_val = ReadTextFile(\"./trees/dev.txt\")\n",
        "y_test, X_test = ReadTextFile(\"./trees/test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJCKt0qEqCmH",
        "colab_type": "text"
      },
      "source": [
        "We need to turn each sequence of words into tokens that serve as inputs into our model. The `DistilBertTokenizer` object does just that. We can see what the tokenizer does to the first sentence in our training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFBvtdiLqDUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMXqosIgizVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5904ca63-bf8e-45d1-ae33-96a0b3a64f1b"
      },
      "source": [
        "sample_txt = str(X_train[0])\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f' Sentence: {sample_txt}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Sentence: ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n",
            "   Tokens: ['[', \"'\", 'the', \"'\", ',', \"'\", 'rock', \"'\", ',', \"'\", 'is', \"'\", ',', \"'\", 'destined', \"'\", ',', \"'\", 'to', \"'\", ',', \"'\", 'be', \"'\", ',', \"'\", 'the', \"'\", ',', \"'\", '21st', \"'\", ',', \"'\", 'century', \"'\", ',', '\"', \"'\", 's', '\"', ',', \"'\", 'new', \"'\", ',', \"'\", '`', '`', \"'\", ',', \"'\", 'conan', \"'\", ',', '\"', \"'\", \"'\", '\"', ',', \"'\", 'and', \"'\", ',', \"'\", 'that', \"'\", ',', \"'\", 'he', \"'\", ',', '\"', \"'\", 's', '\"', ',', \"'\", 'going', \"'\", ',', \"'\", 'to', \"'\", ',', \"'\", 'make', \"'\", ',', \"'\", 'a', \"'\", ',', \"'\", 'splash', \"'\", ',', \"'\", 'even', \"'\", ',', \"'\", 'greater', \"'\", ',', \"'\", 'than', \"'\", ',', \"'\", 'arnold', \"'\", ',', \"'\", 'schwarz', '##ene', '##gger', \"'\", ',', \"'\", ',', \"'\", ',', \"'\", 'jean', '-', 'cl', '##aud', \"'\", ',', \"'\", 'van', \"'\", ',', \"'\", 'dam', '##me', \"'\", ',', \"'\", 'or', \"'\", ',', \"'\", 'steven', \"'\", ',', \"'\", 'sega', '##l', \"'\", ',', \"'\", '.', \"'\", ']']\n",
            "Token IDs: [1031, 1005, 1996, 1005, 1010, 1005, 2600, 1005, 1010, 1005, 2003, 1005, 1010, 1005, 16036, 1005, 1010, 1005, 2000, 1005, 1010, 1005, 2022, 1005, 1010, 1005, 1996, 1005, 1010, 1005, 7398, 1005, 1010, 1005, 2301, 1005, 1010, 1000, 1005, 1055, 1000, 1010, 1005, 2047, 1005, 1010, 1005, 1036, 1036, 1005, 1010, 1005, 16608, 1005, 1010, 1000, 1005, 1005, 1000, 1010, 1005, 1998, 1005, 1010, 1005, 2008, 1005, 1010, 1005, 2002, 1005, 1010, 1000, 1005, 1055, 1000, 1010, 1005, 2183, 1005, 1010, 1005, 2000, 1005, 1010, 1005, 2191, 1005, 1010, 1005, 1037, 1005, 1010, 1005, 17624, 1005, 1010, 1005, 2130, 1005, 1010, 1005, 3618, 1005, 1010, 1005, 2084, 1005, 1010, 1005, 7779, 1005, 1010, 1005, 29058, 8625, 13327, 1005, 1010, 1005, 1010, 1005, 1010, 1005, 3744, 1011, 18856, 19513, 1005, 1010, 1005, 3158, 1005, 1010, 1005, 5477, 4168, 1005, 1010, 1005, 2030, 1005, 1010, 1005, 7112, 1005, 1010, 1005, 16562, 2140, 1005, 1010, 1005, 1012, 1005, 1033]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z27-S1m5CsQ1",
        "colab_type": "text"
      },
      "source": [
        "The model needs to account for a few special tokens, namely the start + end of a sentence, unknown words and lastly for padding (each sentence has a different length, not well suited to feed into batches for a deep learning model so we set a suitable max length, then pad shorter sentences up to that length with a padding token.)  All this word is done for us using the `encode_plus` method, which we use to build our `Dataset` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq2tdKD4sFve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SST_Dataset(Dataset):\n",
        "    def __init__(self, ys, Xs, tokenizer, max_len):\n",
        "        self.targets = ys\n",
        "        self.reviews = Xs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        target = self.targets[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "          review,\n",
        "          add_special_tokens=True,\n",
        "          max_length=self.max_len,\n",
        "          return_token_type_ids=False,\n",
        "          pad_to_max_length=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt',\n",
        "          truncation=True\n",
        "        )\n",
        "        return {\n",
        "          'review_text': review,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLK8xrIJDPnu",
        "colab_type": "text"
      },
      "source": [
        "Next we create our `Dataloader` objects for training, validation and testing. For each item in the dataset we need the encoded input tokens, masks for where the sentence is not padded and the target value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9NJsaFAsbov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_loader(ys, Xs, tokenizer, max_len, batch_size):\n",
        "    ds = SST_Dataset(ys, Xs, tokenizer, max_len)\n",
        "    return DataLoader(ds, batch_size=batch_size)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 128\n",
        "\n",
        "train_data_loader = create_data_loader(y_train, X_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(y_val, X_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(y_test, X_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh9qCmCXCDDt",
        "colab_type": "text"
      },
      "source": [
        "## Constructing our model\n",
        "\n",
        "Now we'ready to build our simple sentiment classification model: we use the output of the `DistilBertModel` - of size 768 - as input into a single fully-connected layer. Dropout is important here for a model with so many parameters (discussed below). (Hugging Face also provide some inbuilt models for downstream tasks that we could have used such as `BertForSequenceClassification` or `BertForQuestionAnswering`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO93UREFyktm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, n_classes=5):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output = self.bert(input_ids, attention_mask)\n",
        "    output= output[0][:,0]\n",
        "    output = self.drop(output)\n",
        "    return self.fc(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8DGYeAackFa",
        "colab_type": "text"
      },
      "source": [
        "The BERT authors had some recommendations for hyperparameters when it comes to fine-tuning:\n",
        "\n",
        "*   *Batch size*: 16, 32\n",
        "*   *Learning rate (Adam)*: 5e-5, 3e-5, 2e-5\n",
        "*   *Number of epochs*: 2, 3, 4\n",
        "\n",
        "We'll largely stick with these - note that the number of epochs is a lot lower than you might expect for a Deep Learning model. This is since we can easily overfit to the training set with many parameters. We'll check for this by calculating both the training and validation accuracy at each epoch. You can find out more about the Hugging Face's optimisers [here](https://huggingface.co/transformers/main_classes/optimizer_schedules.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM9YYbn1Cx27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialise model\n",
        "model = SentimentClassifier()\n",
        "\n",
        "EPOCHS = 5\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader)*EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=50,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkivjBtHElVF",
        "colab_type": "text"
      },
      "source": [
        "Let’s continue with writing our helper functions for training our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GKKUTj-I3nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evalModel(model, data_loader, loss_fn, N):\n",
        "    \"\"\"Evaluate loss and accuracy of model on data_loader\"\"\"\n",
        "    # set model to evaluation mode\n",
        "    model = model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            # get inputs and target \n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            # pass through model + make prediction\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            # update counters\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            correct += (pred == targets).sum().item()\n",
        "            total_loss += loss.item()*len(targets)\n",
        "\n",
        "    # normalise\n",
        "    return 100*correct/N, total_loss/N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IFnHA0vEJoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModel(model, trainDataLoader, valDataLoader, loss_fn, optimizer, scheduler, verbose=True):\n",
        "    \"\"\"Train sentiment classifier\"\"\"\n",
        "    # structure to store progress of the model at each epoch\n",
        "    history = defaultdict(list)\n",
        "    \n",
        "    # move the model to the gpu\n",
        "    model = model.to(device)\n",
        "\n",
        "    for ep in range(EPOCHS):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        # set model to train mode so dropout and batch normalisation layers work as expected\n",
        "        model.train()\n",
        "\n",
        "        for d in trainDataLoader:\n",
        "            # get inputs for batch\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            # calculate output + loss\n",
        "            model.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs.squeeze(), targets.long())\n",
        "\n",
        "            # take gradient step\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # update losses\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "            correct += (pred == targets).sum().item()\n",
        "            total_loss += loss.item()*len(targets)\n",
        "\n",
        "        #after each epoch, collect statistics\n",
        "        history['train_acc'].append(100*correct/len(X_train))\n",
        "        history['train_loss'].append(total_loss/len(X_train))\n",
        "\n",
        "        # statistics about the validation set\n",
        "        val_acc, val_loss = evalModel(model, valDataLoader, loss_fn, len(X_val))\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['vall_loss'].append(val_loss)\n",
        "\n",
        "        #if validation improved, save new best model\n",
        "        if history['val_acc'][-1] == max(history['val_acc']):\n",
        "            print (\"=> Saving a new best at epoch:\", ep)\n",
        "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        \n",
        "        if verbose:\n",
        "            print('Epoch {}/{}'.format(ep+1, EPOCHS))\n",
        "            print('-' * 10)\n",
        "            print('Train loss {} accuracy {}'.format(history['train_loss'][-1], history['train_acc'][-1]))\n",
        "            print('Val loss {} accuracy {}'.format(val_loss, val_acc))\n",
        "\n",
        "    #clean up\n",
        "    model = model.to(torch.device(\"cpu\"))\n",
        "    del input_ids, attention_mask, targets, outputs, _, pred\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAC5l3DTg8Qm",
        "colab_type": "text"
      },
      "source": [
        "Let's train our model and see how it does on our test set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrpUbMLrPFGl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "31a027fc-bf85-4de7-c45d-75e85eef277f"
      },
      "source": [
        "%%time\n",
        "best_model, histories = trainModel(model, train_data_loader, val_data_loader, loss_fn, optimizer, scheduler, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> Saving a new best at epoch: 0\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train loss 1.1928647710812672 accuracy 47.588951310861425\n",
            "Val loss 1.3399161666658768 accuracy 41.416893732970024\n",
            "=> Saving a new best at epoch: 1\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train loss 1.0168351954623553 accuracy 55.77013108614232\n",
            "Val loss 1.3073733697470267 accuracy 44.23251589464124\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train loss 0.8418751696633936 accuracy 64.68867041198502\n",
            "Val loss 1.4277723928563277 accuracy 43.23342415985468\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train loss 0.7150073092472687 accuracy 71.54728464419476\n",
            "Val loss 1.6733300179161883 accuracy 42.143505903723884\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train loss 0.6793975326396553 accuracy 73.67743445692884\n",
            "Val loss 1.7342694530045304 accuracy 41.87102633969119\n",
            "CPU times: user 10min 30s, sys: 5min 42s, total: 16min 13s\n",
            "Wall time: 16min 16s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lD9Eshlh3dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_acc, test_loss = evalModel(best_model.to(device), test_data_loader, loss_fn, len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7AdreD3ODCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b888c7e-ffe0-4f9b-a4a6-2b4962dc089c"
      },
      "source": [
        "print(test_acc, test_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40.85972850678733 1.7384965674370123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RXwFFAeSjx",
        "colab_type": "text"
      },
      "source": [
        "There we have it! We've fine-tuned DistilBert for the task of sentiment classification to over 40% test accuracy in only 5 epochs. We can see that the pre-training step of this Tranformer model produces versatile, useful and high-quality features representing different semantics of language.\n",
        "\n",
        "However we note that this doesn't get us close to [state-of-the-art](https://paperswithcode.com/sota/sentiment-analysis-on-sst-5-fine-grained) on this dataset (55%) - the important lesson here is that we haven't tuned any hyperparameters so finding the best optimizer, learning-rate, droupout amount, adding hidden-layers + number of epochs is what will improve our model. We use the validation set to see what hyperparameters get the best accuracy on that - this estimates how our model will generalise to the unseen test set (see your favourite Learning Theory textbooka as to why this works).\n",
        "\n",
        "Remember that during training we're trying to find the optima a (> 66,000,000 dimension) hypersurface - there's going to many minima so finding the best one requires some searching. Hyperparameter tuning is an important part of solving any problem with Machine Learning, one you just can't avoid.\n",
        "\n",
        "As a final bit of fun, let's see what our model predicts on some raw text - we need to tokenise our custom input then pass it through our trained classifier. Though not a 5 we see the model can correctly identify the review as positive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L8yFZfSc3bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_text = \"I really loved Gladiator! It's my favourite film of all time. The cast was chosen perfectly.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IlPQbAdc4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        "  truncation=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUn9i-SAc7UF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b8bb3a77-9c91-4b60-d4f3-3d8be9bf72a3"
      },
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {int(prediction.cpu().detach().numpy())}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review text: I really loved Gladiator! It's my favourite film of all time. The cast was chosen perfectly.\n",
            "Sentiment  : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpuRad1Sk13e",
        "colab_type": "text"
      },
      "source": [
        "## References & Helpful resources\n",
        "\n",
        "*   [Visualising Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) post by Jay Allamar and its follow up [The Illustrated Tranformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "*   [State of transfer learning in NLP](https://ruder.io/state-of-transfer-learning-in-nlp/)\n",
        "* [Lecture](https://www.youtube.com/watch?v=5vcj8kSwBCY) at Stanford, also found [this](https://youtu.be/S27pHKBEp30) video helpful\n",
        "* The Hugging face Transformer library [docs](https://huggingface.co/transformers/)\n",
        "\n"
      ]
    }
  ]
}